{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합(Overfitting) 을 막는 방법\n",
    "\n",
    "# 모델이 과적합되면 훈련 데이터에 대한 정확도는 높아도 검증 데이터나 테스트 데이터에 대해서는 제대로 동작 X\n",
    "# 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태\n",
    "# 모델의 과적합을 막을 수 있는 여러가지 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터의 양을 늘리기\n",
    "\n",
    "# 데이터의 양을 늘릴수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지\n",
    "# 데이터 양이 적을 경우 의도적으로 기존의 데이터를 조금씩 변형하고 추가: 데이터의 증식 또는 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델의 복잡도 줄이기\n",
    "\n",
    "# 과적합 현상이 포착되었을 때, 인공신경망 모델에 대해서 할 수 있는 한가지 조치는 인공신경망의 복잡도를 줄이는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 규제(Regularization) 적용하기\n",
    "\n",
    "# 복잡한 모델이 간단한 모델보다 과적합될 가능성이 높다\n",
    "# 간단한 모델: 적은 수의 매개변수를 가진 모델을 말한다\n",
    "# 복잡한 모델을 좀 더 간단하게 하는 방법 : 가중치 규제\n",
    "\n",
    "# L1 규제 : 가중치 w들의 절대값 합계를 비용 함수에 추가. L1 노름이라고도 한다\n",
    "# L2 규제 : 모든 가중치 w들의 제곱합을 비용 함수에 추가. L2 노름이라고도 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1, L2 모두 비용 함수를 최소화하기 위해서는 가중치 w 들의 값이 작아져야한다는 특징이 있다.\n",
    "\n",
    "# L1 : L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 한다\n",
    "# 이렇게 되면 가중치 w 의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성은 모델을 만들 때 거의 사용하지 않게 된다\n",
    "\n",
    "# L2 : 가중치의 제곱을 최소화하므로 w 는 완전히 0이 되기 보다는 0에 가까워지는 경향\n",
    "\n",
    "# L1 : 어떤 특성이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용\n",
    "# l2 : 경험적으로 L2 가 더 잘 동작, 가중치 감쇠(Weight decay) 라고도 부른다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치에서의 코드\n",
    "# weight_decay 매게변수를 설정하여 L2 규제 적용\n",
    "\n",
    "model = Architecture1(10, 20, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 1e-4, weight_decay= 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 드롭아웃(Dropout)\n",
    "\n",
    "# 학습 과정에서 신경망의 일부를 사용하지 않는 방법\n",
    "\n",
    "# 예를 들어 드롭아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용\n",
    "# 신경망의 학습 시에만 사용하고, 예측시에는 사용하지 않는 것이 일반적\n",
    "# 학습 시 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지, 매번 랜덤 선택으로 뉴런을 사용하지 않음\n",
    "# 서로 다른 신경망들을 앙상블하여 사용하는 것과 같은 효과를 내어 과적합 방지"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48bde174e9cd0039e04545d8ab8de3430bce749efc257b14db65b6584328569a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
